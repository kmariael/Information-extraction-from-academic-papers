# -*- coding: utf-8 -*-
"""Information_Extraction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z0wvuQKFt0QqZTPv_4Ao2uEvWGGJJEB0

# Initializations
"""

!nvcc --version

!pip install spacy==3.3
!pip install 'spacy[transformers,cuda_111]'

from google.colab import drive
drive.mount('/content/drive')

"""# Add metadata to annotations"""

import pandas as pd

path1 = pd.read_json("/content/ner_1617.jsonl", lines=True)
df1 = pd.DataFrame(path1)

path2 = pd.read_json("/content/full_cleaned_20000-23999bb.jsonl", lines=True)
df2 = pd.DataFrame(path2)

df1.head(5)

df2.head(5)

inner_merged_total = pd.merge(df1, df2, on=["text"])

inner_merged_total

new_data = inner_merged_total.drop(['_input_hash', '_task_hash', '_view_id', 'tokens', '_timestamp'], axis=1)

new_new_data =new_data.rename(columns={'spans_x': 'spans'})

new_new_data

new_data.to_json("data.json", orient='records')

import json
with open('/content/data.json', 'r') as f:
    json_data = json.load(f)
    
with open('METHOD_ACTIVITY_GOAL_RESULT.jsonl', 'w') as outfile:
    for entry in json_data:
        json.dump(entry, outfile)
        outfile.write('\n')

"""# Data Analysis and Manipulation

## Number of sentences in the dataset
"""

def count_sents(filename):
  num = sum(1 for line in open(filename))
  print("Total number of sentences is",num)

count_sents("/content/drive/MyDrive/full_cleaned.jsonl")

"""## Number of papers in the dataset"""

import srsly

def count_papers(filename):
  data = srsly.read_jsonl(filename)
  papers = []
  for col in data:
    tasks = col["meta"]["id"]
    if tasks not in papers:
      papers.append(tasks)
  print("The sentences come from",len(papers), "papers")


count_papers("/content/abstract-sents.jsonl")

"""## Number of tokens in the dataset"""

import srsly
from nltk.tokenize import WordPunctTokenizer

def total_no_tokens(filename):
  data = srsly.read_jsonl(filename)
  tokens = []
  for col in data:
    tok = WordPunctTokenizer().tokenize(col['text'])
    for w in tok:
      tokens.append(w)
  print("Total number of tokens is",len(tokens))

total_no_tokens("/content/ASSERTION.jsonl")

"""## Number of sentences per entity"""

import pandas as pd

def count_sents_per_label(filename, label):
  data = pd.read_json(filename, lines=True)
  sum_sents_label = []
  for idx, row in data.iterrows():
      for s in row.spans:
          if s["label"] == label:
            if row.text not in sum_sents_label:
              sum_sents_label.append(row.text)
  print("The number of sentences that contain", label,"is", len(sum_sents_label))

count_sents_per_label("/content/annotations_meta_no_duplicates.jsonl","ACTIVITY")
count_sents_per_label("/content/annotations_meta_no_duplicates.jsonl","GOAL")

"""## Number of label occurances in the dataset"""

import pandas as pd

def count_label(filename, label):
  data = pd.read_json(filename, lines=True)
  sum_label = 0
  for idx, row in data.iterrows():
    for s in row.spans:
      if s["label"] == label:
        sum_label += 1
  print("This label", label ,"occurs",sum_label, "times")

count_label("/content/annotations_meta_no_duplicates.jsonl", "ACTIVITY")
count_label("/content/annotations_meta_no_duplicates.jsonl","GOAL")

"""## Number of labeled tokens per entity"""

def labeled_tokens_per_entity(path, label):
  data = pd.read_json(path, lines=True)
  spans = []

  for idx, row in data.iterrows():
    for span in row.spans:
        if span['label'] == label:
            spans.append((int(span['token_start']), int(span['token_end'])))
  to_sum = []
  for element in spans:
    a = element[1]-element[0]
    to_sum.append(a)
  print(sum(to_sum))

labeled_tokens_per_entity("/content/annotations_meta_no_duplicates.jsonl","ACTIVITY")

"""## Filter annotations by answer (remove rejected annotations)"""

import os
import srsly

def filter_annotations(dir_path, answer="reject"):
    tasks = []
    texts = [] 
    data = srsly.read_jsonl(dir_path)
    for task in data:
          task["spans"] = [span for span in task["spans"]]
          if task["answer"] != answer and task["text"] not in texts:
              tasks.append(task)
              texts.append(task["text"])
    srsly.write_jsonl(f"_{len(tasks)}.jsonl", tasks)
    return tasks
    

dir_path = "/content/1847_full_cleaned_20000-23999bb_A-based_predsOLD_ner_858.jsonl"
tasks = filter_annotations(dir_path)

"""## Delete duplicate sentences (if any)"""

import pandas as pd

path = pd.read_json("/content/merge1.jsonl", lines=True)
df1 = pd.DataFrame(path)

df1.head()

data = df1.drop_duplicates(subset="text", keep="first", ignore_index= True)
new_data = pd.DataFrame(data)
new_data

new_data.to_json("data.json", orient='records')

import json
with open('/content/data.json', 'r') as f:
    json_data = json.load(f)
    
with open('annotations_meta_no_duplicates.jsonl', 'w') as outfile:
    for entry in json_data:
        json.dump(entry, outfile)
        outfile.write('\n')

"""## Split annotations per entity

### ACTIVITY
"""

import srsly

path="/content/ner_ACTIVITY_METHOD_GOAL_RESULT_ACTIMPLIED_9816.jsonl"
data = srsly.read_jsonl(path)
examples = []
for col in data:
  for item in col['spans']:
    if item["label"] == "ACTIVITY":
      spans = [{"start": item["token_start"], "end": item["token_end"], "label": item["label"]}]
      examples.append({"text": col["text"], "spans": spans})
      srsly.write_jsonl("only_ACT.jsonl", examples)

#create dataframe to combine data with same column "text"
import pandas as pd

path = pd.read_json("only_ACT.jsonl", lines=True)
df = pd.DataFrame(path)
df

merged = df.groupby(['text'], as_index=False)['spans'].sum()
new_df = pd.DataFrame(merged)
new_df

#save dataframe to json file
new_df.to_json("export_df.json", orient='records')#convert json to jsonl
import json

with open('/content/export_df.json', 'r') as f:
    json_data = json.load(f)
    
with open('only_ACT_final.jsonl', 'w') as outfile:
    for entry in json_data:
        json.dump(entry, outfile)
        outfile.write('\n')

"""### GOAL"""

import srsly

path="/content/ner_ACTIVITY_METHOD_GOAL_RESULT_ACTIMPLIED_9816.jsonl"
data = srsly.read_jsonl(path)
examples = []
for col in data:
  for item in col['spans']:
    if item["label"] == "GOAL":
      spans = [{"start": item["start"], "end": item["end"], "label": item["label"]}]
      examples.append({"text": col["text"], "spans": spans, 'meta':col["meta"]})
      srsly.write_jsonl("only_GOAL.jsonl", examples)

#create dataframe to combine data with same column "text"
import pandas as pd

path = pd.read_json("/content/only_GOAL.jsonl", lines=True)
df = pd.DataFrame(path)
df

merged = df.groupby(['text'], as_index=False)['spans'].sum()
new_df = pd.DataFrame(merged)
new_df

#save dataframe to json file
new_df.to_json("export_df.json", orient='records')#convert json to jsonl
import json

with open('/content/export_df.json', 'r') as f:
    json_data = json.load(f)
    
with open('only_GOAL_final.jsonl', 'w') as outfile:
    for entry in json_data:
        json.dump(entry, outfile)
        outfile.write('\n')

"""### No label sentences"""

import srsly

path="/content/ner_ACTIVITY_METHOD_GOAL_RESULT_ACTIMPLIED_9816.jsonl"
data = srsly.read_jsonl(path)
spans = []
examples = []
for col in data:
    if len(col['spans']) == 0:
      examples.append({"text": col["text"], "spans": spans})
      srsly.write_jsonl("no_labels.jsonl", examples)

"""# Machine Learning process

## Spacy NER model (baseline)
"""

!python -m spacy init fill-config base_config.cfg config.cfg

!python -m spacy train config.cfg --output ./output-dir --paths.train /content/Spacy/train.spacy --paths.dev /content/Spacy/dev.spacy

#evaluation
!python -m spacy evaluate ./output-dir/model-best /content/Spacy/eval.spacy --output ner_ACT.jsonl

"""## Custom Word Embeddings

### Import all libraries which will be used
"""

import nltk
nltk.download('punkt')

import gensim
from gensim.models import Word2Vec
from gensim.models.keyedvectors import KeyedVectors

import srsly

import spacy

"""### Word Embeddings from paper abstracts

####Create embeddings
"""

data = srsly.read_jsonl("./abstract-sents.jsonl")
sum_sents = []
for col in data:
      sent = col["text"]
      sum_sents.append(sent)

sum_lists =[]
for sent in sum_sents:
  prep_text = gensim.utils.simple_preprocess(sent, deacc=False, min_len=2, max_len=30)
  sum_lists.append(prep_text)

#train
model = Word2Vec(sentences= sum_lists, sg=1, hs=1, vector_size=300, workers=12, epochs=5)
# save model
model.wv.save_word2vec_format('JSTOR_abstracts_sents_300.txt')
# load model
KeyedVectors.load_word2vec_format("JSTOR_abstracts_sents_300.txt", binary=False)

"""#### ML with embeddings from abstracts"""

import spacy

# init vectors using the txt embeddings file 
!python -m spacy init vectors en /content/drive/MyDrive/MODELS_W2V/JSTOR_abstracts_sents_300.txt /content/abs --name JSTOR_abs_model

!python -m spacy init config -p ner -o accuracy ner.cfg

nlp = spacy.load("./abs")
#nlp.add_pipe("ner")
nlp.to_disk("./abs")

!python -m spacy train /content/abs/ner.cfg --paths.train ./Spacy/train.spacy --paths.dev ./Spacy/dev.spacy --output ./abs/ --paths.vectors ./abs -g 0

!python -m spacy evaluate ./abs/model-best ./Spacy/eval.spacy --output ner_ACT_with_abs_vectors.jsonl -g 0

"""###  Word Embeddings from uncleaned text

#### Create embeddings
"""

import os 

dir_path = "/content/drive/MyDrive/for_w2v"

sum_sents = []
files = os.listdir(dir_path)
for file in files:
      data = srsly.read_jsonl(dir_path+"/"+file)
      for col in data:
        sent = col["text"]
        sum_sents.append(sent)

sum_lists =[]
for sent in sum_sents:
  prep_text = gensim.utils.simple_preprocess(sent, deacc=False, min_len=2, max_len=25)
  sum_lists.append(prep_text)

#train model
model = Word2Vec(sentences= sum_lists, sg=1, hs=1, vector_size=300, workers=12, epochs=5)
# save model
model.wv.save_word2vec_format('JSTOR_uncleaned_sents_model.txt')
# load model
KeyedVectors.load_word2vec_format("JSTOR_uncleaned_sents_model.txt", binary=False)

"""#### ML with embeddings from uncleaned text"""

import spacy

# init vectors using the txt embeddings file 
!python -m spacy init vectors en /content/drive/MyDrive/MODELS_W2V/JSTOR_uncleaned_sents_model_300.txt ./uncl_txt --name JSTOR_unlceaned_sents_model

!python -m spacy init config -p ner -o accuracy ner.cfg

nlp = spacy.load("./uncl_txt")
#nlp.add_pipe("ner")
nlp.to_disk("./uncl_txt")

!python -m spacy train /content/uncl_txt/ner.cfg --paths.train ./Spacy/train.spacy --paths.dev ./Spacy/dev.spacy --output ./uncl_txt --paths.vectors ./uncl_txt -g 0

!python -m spacy evaluate /content/uncl_txt/model-best ./Spacy/eval.spacy --output ner_with_uncleaned_sents_vectors.jsonl -g 0

"""### Word Embeddings from cleaned texts

#### Create embeddings
"""

dir_path = "/content/drive/MyDrive/for_w2v/cleaned_sents"

sum_sents = []
files = os.listdir(dir_path)
for file in files:
      data = srsly.read_jsonl(dir_path+"/"+file)
      for col in data:
        sent = col["text"]
        sum_sents.append(sent)

sum_lists =[]
for sent in sum_sents:
  prep_text = gensim.utils.simple_preprocess(sent, deacc=False, min_len=2, max_len=25)
  sum_lists.append(prep_text)

model = Word2Vec(sentences= sum_lists, sg=1, hs=1, vector_size=300, workers=12, epochs=5)
# save model
model.wv.save_word2vec_format('JSTOR_uncleaned_sents_model.txt')
# load model
KeyedVectors.load_word2vec_format("JSTOR_uncleaned_sents_model.txt", binary=False)

"""#### ML with embeddings from cleaned text"""

import spacy

!python -m spacy init vectors en /content/drive/MyDrive/MODELS_W2V/JSTOR_cleaned_sents_model_300.txt ./cl_txt --name JSTOR_cleaned_model

!python -m spacy init config -p ner -o accuracy ner.cfg

nlp = spacy.load("./cl_txt")
nlp.to_disk("./cl_txt")

!python -m spacy train /content/cl_txt/ner.cfg --paths.train ./Spacy/train.spacy --paths.dev ./Spacy/dev.spacy --output ./cl_txt/ --paths.vectors ./cl_txt -g 0

!python -m spacy evaluate ./cl_txt/model-best ./Spacy/eval.spacy --output ner_with_cleaned_sents_vectors.jsonl -g 0

"""## Deep Learning using BERT & ROBERTa language models """

!python -m spacy init fill-config /content/base_config_trf_bert.cfg /content/config-v3.4.cfg

!python -m spacy train /content/config-v3.4.cfg --paths.train /content/Spacy/train.spacy --paths.dev /content/Spacy/dev.spacy --output /content/ACTIVITY_3785_bert -g 0

!python -m spacy evaluate /content/ACTIVITY_3785_bert/model-best ./Spacy/eval.spacy --output ner_transformers.jsonl -g 0